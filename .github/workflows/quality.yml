name: Model Quality Gates

on:
  push:
    branches: [ main ]
    paths:
      - 'classifier.py'
      - 'data_generator.py'
      - 'evaluator.py'
      - 'spec_augmentation.py'
  workflow_dispatch:  # Allow manual triggering

jobs:
  model-evaluation:
    name: Model Validation Tests
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest
          pip install -r requirements.txt
          
      - name: Download test data
        run: |
          # This would typically download a small test dataset from a storage bucket
          # For demonstration purposes, we'll create dummy data
          mkdir -p test_data
          python -c "import numpy as np; np.save('test_data/dummy_data.npy', np.random.random((10, 95, 126, 3)))"
          
      - name: Run model validation tests
        run: |
          pytest tests/model/ -v
          
      - name: Archive test results
        uses: actions/upload-artifact@v3
        with:
          name: test-results
          path: |
            test-results/
            coverage.xml

  performance-tracking:
    name: Track Model Performance
    runs-on: ubuntu-latest
    needs: model-evaluation
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install matplotlib pandas seaborn
          
      - name: Update performance metrics
        run: |
          # This script would update a metrics database/file with current performance
          python utils/metrics_tracker.py --commit ${{ github.sha }} --save-metrics
          
      - name: Generate performance trend visualization
        run: |
          # This script would generate trend charts from historical data
          python utils/metrics_tracker.py --plot-trends
          
      - name: Archive performance reports
        uses: actions/upload-artifact@v3
        with:
          name: performance-reports
          path: reports/performance/